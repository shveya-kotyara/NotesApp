'use strict';

var BB = require('bluebird');

var contentPath = require('./content/path');

var figgyPudding = require('figgy-pudding');

var finished = BB.promisify(require('mississippi').finished);

var fixOwner = require('./util/fix-owner');

var fs = require('graceful-fs');

var glob = BB.promisify(require('glob'));

var index = require('./entry-index');

var path = require('path');

var rimraf = BB.promisify(require('rimraf'));

var ssri = require('ssri');

BB.promisifyAll(fs);
var VerifyOpts = figgyPudding({
  concurrency: {
    "default": 20
  },
  filter: {},
  log: {
    "default": {
      silly: function silly() {}
    }
  }
});
module.exports = verify;

function verify(cache, opts) {
  opts = VerifyOpts(opts);
  opts.log.silly('verify', 'verifying cache at', cache);
  return BB.reduce([markStartTime, fixPerms, garbageCollect, rebuildIndex, cleanTmp, writeVerifile, markEndTime], function (stats, step, i) {
    var label = step.name || "step #".concat(i);
    var start = new Date();
    return BB.resolve(step(cache, opts)).then(function (s) {
      s && Object.keys(s).forEach(function (k) {
        stats[k] = s[k];
      });
      var end = new Date();

      if (!stats.runTime) {
        stats.runTime = {};
      }

      stats.runTime[label] = end - start;
      return stats;
    });
  }, {}).tap(function (stats) {
    stats.runTime.total = stats.endTime - stats.startTime;
    opts.log.silly('verify', 'verification finished for', cache, 'in', "".concat(stats.runTime.total, "ms"));
  });
}

function markStartTime(cache, opts) {
  return {
    startTime: new Date()
  };
}

function markEndTime(cache, opts) {
  return {
    endTime: new Date()
  };
}

function fixPerms(cache, opts) {
  opts.log.silly('verify', 'fixing cache permissions');
  return fixOwner.mkdirfix(cache, cache).then(function () {
    // TODO - fix file permissions too
    return fixOwner.chownr(cache, cache);
  }).then(function () {
    return null;
  });
} // Implements a naive mark-and-sweep tracing garbage collector.
//
// The algorithm is basically as follows:
// 1. Read (and filter) all index entries ("pointers")
// 2. Mark each integrity value as "live"
// 3. Read entire filesystem tree in `content-vX/` dir
// 4. If content is live, verify its checksum and delete it if it fails
// 5. If content is not marked as live, rimraf it.
//


function garbageCollect(cache, opts) {
  opts.log.silly('verify', 'garbage collecting content');
  var indexStream = index.lsStream(cache);
  var liveContent = new Set();
  indexStream.on('data', function (entry) {
    if (opts.filter && !opts.filter(entry)) {
      return;
    }

    liveContent.add(entry.integrity.toString());
  });
  return finished(indexStream).then(function () {
    var contentDir = contentPath._contentDir(cache);

    return glob(path.join(contentDir, '**'), {
      follow: false,
      nodir: true,
      nosort: true
    }).then(function (files) {
      return BB.resolve({
        verifiedContent: 0,
        reclaimedCount: 0,
        reclaimedSize: 0,
        badContentCount: 0,
        keptSize: 0
      }).tap(function (stats) {
        return BB.map(files, function (f) {
          var split = f.split(/[/\\]/);
          var digest = split.slice(split.length - 3).join('');
          var algo = split[split.length - 4];
          var integrity = ssri.fromHex(digest, algo);

          if (liveContent.has(integrity.toString())) {
            return verifyContent(f, integrity).then(function (info) {
              if (!info.valid) {
                stats.reclaimedCount++;
                stats.badContentCount++;
                stats.reclaimedSize += info.size;
              } else {
                stats.verifiedContent++;
                stats.keptSize += info.size;
              }

              return stats;
            });
          } else {
            // No entries refer to this content. We can delete.
            stats.reclaimedCount++;
            return fs.statAsync(f).then(function (s) {
              return rimraf(f).then(function () {
                stats.reclaimedSize += s.size;
                return stats;
              });
            });
          }
        }, {
          concurrency: opts.concurrency
        });
      });
    });
  });
}

function verifyContent(filepath, sri) {
  return fs.statAsync(filepath).then(function (stat) {
    var contentInfo = {
      size: stat.size,
      valid: true
    };
    return ssri.checkStream(fs.createReadStream(filepath), sri)["catch"](function (err) {
      if (err.code !== 'EINTEGRITY') {
        throw err;
      }

      return rimraf(filepath).then(function () {
        contentInfo.valid = false;
      });
    }).then(function () {
      return contentInfo;
    });
  })["catch"]({
    code: 'ENOENT'
  }, function () {
    return {
      size: 0,
      valid: false
    };
  });
}

function rebuildIndex(cache, opts) {
  opts.log.silly('verify', 'rebuilding index');
  return index.ls(cache).then(function (entries) {
    var stats = {
      missingContent: 0,
      rejectedEntries: 0,
      totalEntries: 0
    };
    var buckets = {};

    for (var k in entries) {
      if (entries.hasOwnProperty(k)) {
        var hashed = index._hashKey(k);

        var entry = entries[k];
        var excluded = opts.filter && !opts.filter(entry);
        excluded && stats.rejectedEntries++;

        if (buckets[hashed] && !excluded) {
          buckets[hashed].push(entry);
        } else if (buckets[hashed] && excluded) {// skip
        } else if (excluded) {
          buckets[hashed] = [];
          buckets[hashed]._path = index._bucketPath(cache, k);
        } else {
          buckets[hashed] = [entry];
          buckets[hashed]._path = index._bucketPath(cache, k);
        }
      }
    }

    return BB.map(Object.keys(buckets), function (key) {
      return rebuildBucket(cache, buckets[key], stats, opts);
    }, {
      concurrency: opts.concurrency
    }).then(function () {
      return stats;
    });
  });
}

function rebuildBucket(cache, bucket, stats, opts) {
  return fs.truncateAsync(bucket._path).then(function () {
    // This needs to be serialized because cacache explicitly
    // lets very racy bucket conflicts clobber each other.
    return BB.mapSeries(bucket, function (entry) {
      var content = contentPath(cache, entry.integrity);
      return fs.statAsync(content).then(function () {
        return index.insert(cache, entry.key, entry.integrity, {
          metadata: entry.metadata,
          size: entry.size
        }).then(function () {
          stats.totalEntries++;
        });
      })["catch"]({
        code: 'ENOENT'
      }, function () {
        stats.rejectedEntries++;
        stats.missingContent++;
      });
    });
  });
}

function cleanTmp(cache, opts) {
  opts.log.silly('verify', 'cleaning tmp directory');
  return rimraf(path.join(cache, 'tmp'));
}

function writeVerifile(cache, opts) {
  var verifile = path.join(cache, '_lastverified');
  opts.log.silly('verify', 'writing verifile to ' + verifile);

  try {
    return fs.writeFileAsync(verifile, '' + +new Date());
  } finally {
    fixOwner.chownr.sync(cache, verifile);
  }
}

module.exports.lastRun = lastRun;

function lastRun(cache) {
  return fs.readFileAsync(path.join(cache, '_lastverified'), 'utf8').then(function (data) {
    return new Date(+data);
  });
}